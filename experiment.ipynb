{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dQ-li_GHOF"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This project was inspired by the paper [Deep Learning for Symbolic Mathematics](https://arxiv.org/abs/1912.01412?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529) by G. Lample and F. Charton. The authors apply a seq2seq transformer model to solve inverse problems in symbolic mathematics, namely the integration of functions in one variable and the solving of first and second order ordinary differential equations.\n",
        "\n",
        "I wanted to see if a transformer would also be successful in factoring polynomials with integer coefficients. Although a deterministic polynomial-time algorithm is known (the 1982 LLL algorithm of Lenstra, Lenstra and Lovasz), it does not perform well in practice. For that reason computer algebra systems such as Mathematica and Maple resort to probabilistic algorithms that perform better on average. It thus seemed appropriate to try to find out how well would a deterministic neural network perform on that task.\n",
        "\n",
        "To limit the scope of the problem, only polynomials with degree at most 8 and with coefficients in the range [-5, 5] were considered. It is also required that such a polynomial be primitive, that is the gcd of its coefficients is 1. This represents a space of some 2B possible polynomials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsGyGfCGQ70b"
      },
      "source": [
        "# Formulation of the problem\n",
        "\n",
        "We consider the space Z[x] of polynomials with integer coefficients, e.g. 1 + 2x + x^2. A polynomial p(x) in Z[x] is called irreducible if no non-constant polynomial other than itself into it. On the other hand a polynomial is called a primitive if the gcd of its coefficients is 1. So for example 1 + 3x^2 is irreducible but 2 + 8x^3 is not.\n",
        "\n",
        "A primitive polynomial can always be expressed as a product of uniuely determined primitive irreducible polynomials. For example, the primitive polynomial -1 - 2x - x^2 + 3x^4 - 2x^5 - x^6 + 4x^7 can be expressed as the product of the primitive polynomials \n",
        "\n",
        "(1 + 2x + 2x^2 + 2x^3 - x^4 + 4x^5)(1 + x)(-1 + x) \n",
        "\n",
        "More generally, we can always factor out the gcd of the coefficients of a polynomial so that the condition of primitivity is met, so the general problem of factoring polynomials in Z[x] reduces to that of factoring primitive polynomials. Within the general project of applying neural networks to symbolic mathematics, I wish to explore how effective would a neural network be in learning to decompose polynomials into their irreducible factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2xdFH-20q0"
      },
      "source": [
        "# Running the experiment\n",
        "\n",
        "We will train a seq2seq transformer model on 20M examples of primitive polynomials of degree at most 8 and with coefficients in the range [-5, 5]. The examples were all generated in Mathematica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZfuEJgLsTpV"
      },
      "source": [
        "import pickle as pk\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import keras\n",
        "import copy\n",
        "from keras_transformer import get_model, get_custom_objects"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xZT_Tl1UuiE"
      },
      "source": [
        "Let's load the training data. It is stored in four seperate files. The training data will consist in two lists: one containing polynomials and the other their decompositions. Polynomials are store as numpy arrays of shape (9,) and decompositions as numpy arrays of shape (9n,) where n is the number of irreducible factors. For example, the polynomial -1 - 2x - x^2 + 3x^4 - 2x^5 - x^6 + 4x^7 we saw above is stored as the array \n",
        "\n",
        "[-1, -2, -1, 0, 3, -2, -1, 4, 0] \n",
        "\n",
        "and its decomposition (1 + 2x + 2x^2 + 2x^3 - x^4 + 4x^5)(1 + x)(-1 + x) \n",
        "is stored as the array \n",
        "\n",
        "[1, 2, 2, 2, -1, 4, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0]. \n",
        "\n",
        "By convention, the irreducible factors are listed in reverse lexicographical order. To recover the individual factors from a (9n,)-shaped array storing a decomposition we need to split it into n equal parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ipdm_BV-gc"
      },
      "source": [
        "for i in range(1, 5):\n",
        "  with open('polys_int_set_{}.pkl'.format(i), 'rb') as file:\n",
        "    globals()['polys_%s' % i], _ = pk.load(file)\n",
        "\n",
        "  with open('decomp_int_set_{}.pkl'.format(i), 'rb') as file:\n",
        "    globals()['decomps_%s' % i], _ = pk.load(file)\n",
        "\n",
        "polys = polys_1 + polys_2 + polys_3 + polys_4\n",
        "decomps = decomps_1 + decomps_2 + decomps_3 + decomps_4\n",
        "\n",
        "for i in range(1, 5):\n",
        "  del  globals()['polys_%s' % i]\n",
        "  del  globals()['decomps_%s' % i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TRYArRUWue"
      },
      "source": [
        "def to_string(poly):\n",
        "  poly = poly.astype('int')\n",
        "  poly = [str(number) for number in poly]\n",
        "  string_poly = [\"\"] * (2*len(poly) - 1)\n",
        "  string_poly[0::2] = poly\n",
        "  string_poly[1::2] = '+'*(len(poly) - 1) \n",
        "  return ''.join(string_poly)\n",
        "\n",
        "def to_int_array(poly):\n",
        "  poly = poly.split('+')\n",
        "  poly = np.asarray([int(number) for number in poly])\n",
        "  return np.trim_zeros(poly, 'b')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLOrVWa1UWug"
      },
      "source": [
        "for index, poly in enumerate(polys):\n",
        "  poly = to_string(poly)\n",
        "  polys[index] = poly\n",
        "\n",
        "for index, de in enumerate(decomps):\n",
        "  splits = int(len(de) / 9)\n",
        "  factors = np.split(de, splits)\n",
        "  factors = [np.trim_zeros(factor, 'b') for factor in factors]\n",
        "  factors = [to_string(factor) for factor in factors]\n",
        "  string_factors = [\"\"]*(2*splits - 1)\n",
        "  string_factors[0::2] = factors\n",
        "  string_factors[1::2] = '|'*(splits - 1)\n",
        "  string_factors = ''.join(string_factors)\n",
        "  decomps[index] = string_factors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAXHXxShUlDL"
      },
      "source": [
        "Now we prepare the training data for training. We will encode the data character by character, treating + and - as label classes. We need to be careful about memory management due to the large size of the data and the 24GB cap of usable RAM in Google Collab pro.\n",
        "\n",
        "Recall that during training seq2seq models require two inputs: the input to the encoder layer, in this case the polynomials we wish to decompose, and the input to the decoder layer which is the target output shifted one timestep into the past. The idea is to train the model to make a prediction for timestep t having seen timesteps 1...t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUFahoAHU4-J"
      },
      "source": [
        "# Define dictionary for encoding\n",
        "token_dict = {}\n",
        "token_dict['pad'] = 0\n",
        "for i in (list(range(0, 10)) + ['+', '-', '|', 'start', 'end']):\n",
        "  token_dict[str(i)] = len(token_dict)\n",
        "rev_token_dict = {v:k for k, v in token_dict.items()}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuQXo0yrU4-K"
      },
      "source": [
        "# determine maximum input and output lengths\n",
        "max_len_decomps = 0\n",
        "for de in decomps:\n",
        "  if len(de) > max_len_decomps:\n",
        "    max_len_decomps = len(de)\n",
        "\n",
        "max_len_polys = 0\n",
        "for poly in polys:\n",
        "  if len(poly) > max_len_polys:\n",
        "    max_len_polys = len(poly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQf-IcW9Uhki"
      },
      "source": [
        "data_len = len(polys)\n",
        "\n",
        "encoder_inputs = np.zeros((data_len, max_len_polys + 2))\n",
        "decoder_inputs = np.zeros((data_len, max_len_decomps + 2))\n",
        "decoder_outputs = np.zeros((data_len, max_len_decomps + 2))\n",
        "\n",
        "for index in range(data_len):\n",
        "  posn = data_len - index - 1\n",
        "\n",
        "  # construct encoder input\n",
        "  poly = polys.pop()\n",
        "  poly = np.asarray([token_dict[char] for char in poly] + \n",
        "                    [token_dict['end']], 'int')\n",
        "  poly = np.pad(poly, (1, max_len_polys + 1 - len(poly)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  encoder_inputs[posn, :] = poly\n",
        "\n",
        "  # construct decoder input and output\n",
        "\n",
        "  # first preprocess decomposition\n",
        "  de = decomps.pop()\n",
        "  de = np.asarray([token_dict[char] for char in de] + \n",
        "                  [token_dict['end']], 'int')\n",
        "  de_1 = np.pad(de, (1, max_len_decomps + 1 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  de_2 = np.pad(de, (0, max_len_decomps + 2 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  decoder_inputs[posn, :] = de_1\n",
        "  decoder_outputs[posn, :] = de_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzbwErjxbJbO"
      },
      "source": [
        "Now we construct model. Hyperparameters were chosen based on standard values suggested in the literature. This is because each training iteration takes several hours and thus hyperparameter-tuning would be prohibitively time-consuming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzhcM_i5twEB",
        "outputId": "8fcf10d8-1b39-412f-da56-305b05f54b37"
      },
      "source": [
        "model = get_model(\n",
        "    token_num=len(token_dict),\n",
        "    embed_dim=512,\n",
        "    encoder_num=6,\n",
        "    decoder_num=6,\n",
        "    head_num=8,\n",
        "    hidden_dim=2048,\n",
        "    attention_activation='relu',\n",
        "    feed_forward_activation='relu')\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Token-Embedding (EmbeddingRet)  [(None, None, 512),  8192        Encoder-Input[0][0]              \n",
            "                                                                 Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Embedding (TrigPosEmbed (None, None, 512)    0           Token-Embedding[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-Embedding[0][0]          \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, None, 512)    0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, None, 512)    0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, None, 512)    0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, None, 512)    0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, None, 512)    0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Embedding (TrigPosEmbed (None, None, 512)    0           Token-Embedding[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-Embedding[0][0]          \n",
            "                                                                 Decoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, None, 512)    0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Add (Add) (None, None, 512)    0           Decoder-1-MultiHeadQueryAttention\n",
            "                                                                 Decoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Add (Add) (None, None, 512)    0           Decoder-2-MultiHeadQueryAttention\n",
            "                                                                 Decoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-3-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-3-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-3-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-FeedForward-Add (Add) (None, None, 512)    0           Decoder-3-MultiHeadQueryAttention\n",
            "                                                                 Decoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-4-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-4-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-4-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-FeedForward-Add (Add) (None, None, 512)    0           Decoder-4-MultiHeadQueryAttention\n",
            "                                                                 Decoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-5-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-5-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-5-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-FeedForward-Add (Add) (None, None, 512)    0           Decoder-5-MultiHeadQueryAttention\n",
            "                                                                 Decoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-6-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-6-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-6-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-FeedForward-Add (Add) (None, None, 512)    0           Decoder-6-MultiHeadQueryAttention\n",
            "                                                                 Decoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Output (EmbeddingSim)   (None, None, 16)     16          Decoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Token-Embedding[1][1]            \n",
            "==================================================================================================\n",
            "Total params: 44,146,704\n",
            "Trainable params: 44,146,704\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qkZCcUJcUmE"
      },
      "source": [
        "The model is trained for just 1 epoch, which takes 10h on Google Collab Pro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RQBbCoMJzan",
        "outputId": "da5722e1-91e5-45df-bde2-2c2e2839bca9"
      },
      "source": [
        "model.fit(\n",
        "    x=[encoder_inputs, decoder_inputs],\n",
        "    y=decoder_outputs,\n",
        "    epochs=1,\n",
        "    batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77344/77344 [==============================] - 20619s 266ms/step - loss: 0.1223 - accuracy: 0.9348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fccc8e76a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvbu4dB2BPJp"
      },
      "source": [
        "del encoder_inputs, decoder_inputs, decoder_ouputs\n",
        "model.save('./model_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unudedtRYR-2"
      },
      "source": [
        "Next, test data is loaded and processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqRVZf-u8ZCZ"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Let's see how will our trained neural network does on never-seen data. We load the test data and preprocess it like before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LflYee35Hh_"
      },
      "source": [
        "for i in range(1, 5):\n",
        "  with open('polys_int_set_{}.pkl'.format(i), 'rb') as file:\n",
        "    _, globals()['test_polys_%s' % i] = pk.load(file)\n",
        "\n",
        "  with open('decomp_int_set_{}.pkl'.format(i), 'rb') as file:\n",
        "    _, globals()['test_decomps_%s' % i] = pk.load(file)\n",
        "\n",
        "test_polys = test_polys_1 + test_polys_2 + test_polys_3 + test_polys_4\n",
        "test_decomps = test_decomps_1 + test_decomps_2 + test_decomps_3 + test_decomps_4\n",
        "for i in range(1, 5):\n",
        "  del  globals()['test_polys_%s' % i]\n",
        "  del  globals()['test_decomps_%s' % i]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYS6G4d-arL7"
      },
      "source": [
        "for index, poly in enumerate(test_polys):\n",
        "  poly = to_string(poly)\n",
        "  test_polys[index] = poly\n",
        "\n",
        "for index, de in enumerate(test_decomps):\n",
        "  splits = int(len(de) / 9)\n",
        "  factors = np.split(de, splits)\n",
        "  factors = [np.trim_zeros(factor, 'b') for factor in factors]\n",
        "  factors = [to_string(factor) for factor in factors]\n",
        "  string_factors = [\"\"]*(2*splits - 1)\n",
        "  string_factors[0::2] = factors\n",
        "  string_factors[1::2] = '|'*(splits - 1)\n",
        "  string_factors = ''.join(string_factors)\n",
        "  test_decomps[index] = string_factors"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auIGF8j38s3K"
      },
      "source": [
        "Let's just make sure that the maximal lengths of our test data does not exceed those seen in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9lXCksoUjXi"
      },
      "source": [
        "max_len_polys_test = 0\n",
        "for poly in test_polys:\n",
        "  if len(poly) > max_len_polys_test:\n",
        "    max_len_polys_test = len(poly)\n",
        "\n",
        "max_len_decomps_test = 0\n",
        "for de in test_decomps:\n",
        "  if len(de) > max_len_decomps_test:\n",
        "    max_len_decomps_test = len(de)\n",
        "\n",
        "assert max_len_polys >= max_len_polys_test\n",
        "assert max_len_decomps >= max_len_decomps_test"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjr7OPHw8_sa"
      },
      "source": [
        "Let's prepare the test data like we did for the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDSD5G4YYBQB"
      },
      "source": [
        "data_len = len(test_polys)\n",
        "\n",
        "test_encoder_inputs = np.zeros((data_len, max_len_polys + 2))\n",
        "test_decoder_inputs = np.zeros((data_len, max_len_decomps + 2))\n",
        "test_decoder_outputs = np.zeros((data_len, max_len_decomps + 2))\n",
        "\n",
        "for index in range(data_len):\n",
        "  posn = data_len - index - 1\n",
        "\n",
        "  # construct encoder input\n",
        "  poly = test_polys.pop()\n",
        "  poly = np.asarray([token_dict[char] for char in poly] + \n",
        "                    [token_dict['end']], 'int')\n",
        "  poly = np.pad(poly, (1, max_len_polys + 1 - len(poly)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  test_encoder_inputs[posn, :] = poly\n",
        "\n",
        "  # construct decoder input and output\n",
        "\n",
        "  # first preprocess decomposition\n",
        "  de = test_decomps.pop()\n",
        "  de = np.asarray([token_dict[char] for char in de] + \n",
        "                  [token_dict['end']], 'int')\n",
        "  de_1 = np.pad(de, (1, max_len_decomps + 1 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  de_2 = np.pad(de, (0, max_len_decomps + 2 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  test_decoder_inputs[posn, :] = de_1\n",
        "  test_decoder_outputs[posn, :] = de_2\n",
        "\n",
        "\n",
        "del test_polys, test_decomps"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiP-A7Ij9nUj"
      },
      "source": [
        "First we test our neural network in the same way we trained it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-II4SZubCwv",
        "outputId": "298f2162-67d4-4060-8431-ee4d0bf62784"
      },
      "source": [
        "model.evaluate([test_encoder_inputs, test_decoder_inputs], test_decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6250/6250 [==============================] - 98s 16ms/step - loss: 0.0050 - accuracy: 0.9981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.004966276697814465, 0.9981300234794617]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVtxteKADCC"
      },
      "source": [
        "Not bad! Next we will test our model at inference, i.e. without any knowledge of the correct decoder inputs. We will do this with beam search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syeC_CYa6oHr"
      },
      "source": [
        "def beam_search(model, src_input, k=1, max_len=max_len_decomps + 2):\n",
        "    # k_beam will be updated to contain best k beams at each timestep\n",
        "    src_input = src_input.reshape((1,max_len_polys + 2))\n",
        "    k_beam = [(0, np.zeros(max_len))]\n",
        "\n",
        "    # l : point on target sentence to predict\n",
        "    for l in range(max_len):\n",
        "        all_k_beams = []\n",
        "        for prob, sent_predict in k_beam:\n",
        "          if token_dict['end'] in sent_predict:\n",
        "            all_k_beams.append((prob, sent_predict))\n",
        "          else:\n",
        "            decoder_input = np.concatenate([[token_dict['start']], \n",
        "                                          sent_predict[:-1]]).reshape(\n",
        "                                              (1,max_len))\n",
        "            predicted = model.predict([src_input, decoder_input])[0]\n",
        "            possible_k = predicted[l].argsort()[-k:][::-1]\n",
        "      \n",
        "            for next_wid in possible_k:\n",
        "              all_k_beams.append(\n",
        "                  (prob - np.log(predicted[l][next_wid]), np.concatenate(\n",
        "                      [sent_predict[:l], np.asarray([next_wid]), np.zeros(\n",
        "                         max_len - l - 1)])))\n",
        "        k_beam = sorted(all_k_beams, key=lambda tup:tup[0])[:k]\n",
        "\n",
        "    return k_beam"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfNkvJwF-iWy"
      },
      "source": [
        "We test the accuracy of the model using different beam sizes at inference on 1000 test inputs. We consider the neural network as having solved a problem if the solution in the final beam.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQCPr-vSSBE"
      },
      "source": [
        "tests = list(zip(test_encoder_inputs, test_decoder_outputs))\n",
        "shuffle(tests)\n",
        "test_encoder_inputs, test_decoder_outputs = zip(*tests)\n",
        "test_encoder_inputs = list(test_encoder_inputs[:1000])\n",
        "test_decoder_outputs = list(test_decoder_outputs[:1000])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "-pI0p5fHCyom",
        "outputId": "7a4e620a-234b-4213-9c9d-779f7ed204a7"
      },
      "source": [
        "for k in [1, 2, 3, 5, 10]:\n",
        "  truth_list = []\n",
        "  for index, poly in enumerate(test_encoder_inputs):\n",
        "    for _, soln in beam_search(model=model, src_input=poly, k=k):\n",
        "      if np.array_equal(test_decoder_outputs[index], soln):\n",
        "        truth_list.append(True)\n",
        "        break\n",
        "    if len(truth_list) < index + 1:\n",
        "      truth_list.append(False)\n",
        "  print('Accuracy with beam size {}:'.format(k),\n",
        "        '{}/{}'.format(truth_list.count(True), len(test_encoder_inputs)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with beam size 1: 983/1000\n",
            "Accuracy with beam size 2: 987/1000\n",
            "Accuracy with beam size 3: 991/1000\n",
            "Accuracy with beam size 5: 992/1000\n",
            "Accuracy with beam size 10: 993/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq8ymmrOpT5I"
      },
      "source": [
        "Accuracy steadily improved with beam size. This is in contrast to NLP applications where increased beam size can sometimes be detrimental to accuracy. Lample and Charton also observed substantial improvements as beam size increased. This suggests that for purpose of symbolic mathematics, higher beam size is generally better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6Au7C6FwYa0"
      },
      "source": [
        "Finally, I wanted to see how well would the model generalize to a range of input characters it has never seen. For that purpose we generate a set of 100 polynomials with coefficients between -9 and 9, making sure that not all coefficients are between -5 and 5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrSkf6ziwyo4"
      },
      "source": [
        "from math import gcd\n",
        "from random import choices\n",
        "\n",
        "def gcd_v(array):\n",
        "  array = array.astype('int')\n",
        "  if len(array) == 2:\n",
        "    return gcd(array[0], array[1])\n",
        "  return gcd(gcd_v(array[:-1]), array[-1])\n",
        "\n",
        "new_polys = []\n",
        "\n",
        "while len(new_polys) < 100:\n",
        "  poly = choices(range(-9, 10), k=9)\n",
        "  if max(poly) <= 5 and min(poly) >= -5:\n",
        "    continue\n",
        "  else:\n",
        "    poly = np.asarray(poly, 'int')\n",
        "    poly = poly / gcd_v(poly)\n",
        "    new_polys.append(poly)\n",
        "\n",
        "# preprocess for feeding to neural network\n",
        "for index, poly in enumerate(new_polys):\n",
        "  poly = to_string(poly)\n",
        "  assert len(poly) <= max_len_polys # may have to run cell more than once\n",
        "  new_polys[index] = poly\n",
        "\n",
        "new_polys_inputs = np.zeros((len(new_polys), max_len_polys + 2))\n",
        "\n",
        "for index, poly in enumerate(new_polys):\n",
        "  posn = len(new_polys) - index - 1\n",
        "  poly = np.asarray([token_dict[char] for char in poly] + \n",
        "                    [token_dict['end']], 'int')\n",
        "  poly = np.pad(poly, (1, max_len_polys + 1 - len(poly)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  new_polys_inputs[posn, :] = poly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOMy7m6FMkpV"
      },
      "source": [
        "We will simply check whether the neural network's solutions are 1. valid and 2. give factors that multiply to the input polynomials, nevermind the irreducibility of the factors. Let's define a function verify_soln() for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ9yV0vRN9Wt"
      },
      "source": [
        "def polymul(p1, p2):\n",
        "  prod_size = len(p1) + len(p2) - 1\n",
        "  p1 = np.pad(p1, (0, prod_size - len(p1)))\n",
        "  p2 = np.pad(p2, (0, prod_size - len(p2)))\n",
        "  prod = np.zeros(prod_size)\n",
        "  for k in range(prod_size):\n",
        "    for j in range(k + 1):\n",
        "      prod[k] += p1[j] * p2[k - j]\n",
        "  prod = np.trim_zeros(prod, 'b')\n",
        "  return prod\n",
        "\n",
        "def is_valid(string_poly):\n",
        "  digits = [str(i) for i in range(0,10)]\n",
        "  if string_poly[-1] not in digits:\n",
        "    return False\n",
        "  elif \"--\" in string_poly or \"++\" in string_poly or \"-+\" in string_poly:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def verify_soln(poly, soln):\n",
        "  # preprocess polynomial\n",
        "  poly = np.trim_zeros(poly, 'b')[1:-1]\n",
        "  poly = poly.tolist()\n",
        "  poly = ''.join([rev_token_dict[member] for member in poly])\n",
        "  poly = to_int_array(poly)\n",
        "\n",
        "  # preprocess solution\n",
        "  soln = np.trim_zeros(soln, 'b')\n",
        "  if soln[-1] != token_dict['end']:\n",
        "    return False\n",
        "  soln = soln[:-1].tolist()\n",
        "  soln = ''.join([rev_token_dict[member] for member in soln])\n",
        "  factors = soln.split('|')\n",
        "  validity = [is_valid(factor) for factor in factors]\n",
        "  if False in validity:\n",
        "    return False\n",
        "  factors = [to_int_array(factor) for factor in factors]\n",
        "\n",
        "  product = np.ones(1)\n",
        "  while len(factors) > 0:\n",
        "    factor = factors.pop()\n",
        "    product = polymul(product, factor)\n",
        "\n",
        "  if np.array_equal(poly, product):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tjEqbX42lh4"
      },
      "source": [
        "Let's try with beam sizes 1, 3 and 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8xQDm4f2XnR",
        "outputId": "f7097487-7b9f-422b-9198-f1909d928f86"
      },
      "source": [
        "for k in [1, 3, 5]:\n",
        "  truth_list = []\n",
        "  for index, poly in enumerate(new_polys_inputs):\n",
        "    for _, soln in beam_search(model=model, src_input=poly, k=k):\n",
        "      if verify_soln(poly, soln):\n",
        "        truth_list.append(True)\n",
        "        break\n",
        "    if len(truth_list) < index + 1:\n",
        "      truth_list.append(False)\n",
        "  print('Accuracy with beam size {}:'.format(k),\n",
        "        '{}/{}'.format(truth_list.count(True), len(new_polys_inputs)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with beam size 1: 0/100\n",
            "Accuracy with beam size 3: 0/100\n",
            "Accuracy with beam size 5: 0/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GLilFDW8hKB"
      },
      "source": [
        "The neural network doesn't generalize at all! This is not too surprising; the embedding layer does not actually \"understand\" numerical quantities and so there is no reason it should generalize to a range of input coefficients it has never seen. This said, there is a caveat: the model would have a better chance at generalization if had seen every digit and positional value at least once, as opposed to only seeing the digits 0-5 and their negative. For instance, it might be able to correctly interpret the number 15 if it sees 10 and 5 during training. However, naively extending the range of input coefficients to [-10, 10] would require a larger training dataset since the input space would be far larger (about 700B possible inputs). I expect that by adding a neural arithmetic logic unit (see [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)) to the model we would see significant improvements to never-seen ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrnFLktCHZ-j"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Athough the neural network performed well on test data drawn from the input coefficient range on which it was trained, it completely failed to generalize to test data drawn from a never-seen coefficient range. It appears to me the issue is that a naive transformer model is not sufficient: we need layers that provide the neural network with a \"sense\" of numeracy. NALUs seem promising in that regard. Furthermore it seems that without such improvements to the transformer model, datasets would need to be extremely large to obtain adequate accuracy over an acceptably large input space.\n",
        "\n",
        "Nonetheless, the fact that the neural network displayed such great accuracy on the restricted input space we considered, this after having seen less than 1% of all possible inputs, is very promising. It swhows that neural networks are indeed capable of learning non-trivial mathematical relationships exceptionally well. I therefore expect them to continue finding applications in symbolic mathematics, even though they are not the panacea they may appear to be at first glance. I am very interested in exploring ways of improving the transformer model for this special use case in the future."
      ]
    }
  ]
}
