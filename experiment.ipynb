{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dQ-li_GHOF"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This project was inspired by the paper [Deep Learning for Symbolic Mathematics](https://arxiv.org/abs/1912.01412?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529) by G. Lample and F. Charton. The authors apply a seq2seq transformer model to solve inverse problems in symbolic mathematics, namely the integration of functions in one variable and the solving of first and second order ordinary differential equations.\n",
        "\n",
        "I wanted to see if a transformer would also be successful in factoring polynomials with integer coefficients. Although a deterministic polynomial-time algorithm is known (the 1982 LLL algorithm of Lenstra, Lenstra and Lovasz), it does not perform well in practice. For that reason computer algebra systems such as Mathematica and Maple resort to probabilistic algorithms that perform better on average. It thus seemed appropriate to try to find out how well would a deterministic neural network perform on that task.\n",
        "\n",
        "To limit the scope of the problem, only polynomials with degree at most 8 and with coefficients in the range [-5, 5] are considered. It is also required that such a polynomial be primitive, that is the gcd of its coefficients is 1. This represents an input space of some 1.2B possible polynomials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsGyGfCGQ70b"
      },
      "source": [
        "# Formulation of the problem\n",
        "\n",
        "We consider the space Z[x] of polynomials with integer coefficients, e.g. 1 + 2x + x^2. A polynomial p(x) in Z[x] is called irreducible if no non-constant polynomial other than itself divides into it. On the other hand a polynomial is called a primitive if the gcd of its coefficients is 1. So for example 1 + 3x^2 is irreducible but 2 + 8x^3 is not.\n",
        "\n",
        "A primitive polynomial can always be expressed as a product of uniuely determined primitive irreducible polynomials. For example, the primitive polynomial -1 - 2x - x^2 + 3x^4 - 2x^5 - x^6 + 4x^7 can be expressed as the product of the primitive polynomials \n",
        "\n",
        "(1 + 2x + 2x^2 + 2x^3 - x^4 + 4x^5)(1 + x)(-1 + x) \n",
        "\n",
        "More generally, we can always factor out the gcd of the coefficients of a polynomial so that the condition of primitivity is met, so the general problem of factoring polynomials in Z[x] reduces to that of factoring primitive polynomials. Within the general project of applying neural networks to symbolic mathematics, I wish to explore how effective would a neural network be in learning to decompose polynomials into their irreducible factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2xdFH-20q0"
      },
      "source": [
        "# Running the experiment\n",
        "\n",
        "We will train a seq2seq transformer model on ~20M samples of primitive polynomials of degree at most 8 and with coefficients in the range [-5, 5]. The samples were generated in Mathematica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZfuEJgLsTpV"
      },
      "source": [
        "import pickle as pk\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import keras.applications, keras.preprocessing, keras.wrappers, keras.datasets\n",
        "from keras_transformer import get_model, get_custom_objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xZT_Tl1UuiE"
      },
      "source": [
        "Let's load the training data. It is stored in four seperate files, each containing a (training_data, test_data) pair, where *_data are lists. The training data will consist in two lists: one containing polynomials and the other their decompositions. Polynomials are store as numpy arrays of shape (9,) and decompositions as numpy arrays of shape (9n,) where n is the number of irreducible factors. For example, the polynomial -1 - 2x - x^2 + 3x^4 - 2x^5 - x^6 + 4x^7 we saw above is stored as the array \n",
        "\n",
        "[-1, -2, -1, 0, 3, -2, -1, 4, 0] \n",
        "\n",
        "and its decomposition (1 + 2x + 2x^2 + 2x^3 - x^4 + 4x^5)(1 + x)(-1 + x) \n",
        "is stored as the array \n",
        "\n",
        "[1, 2, 2, 2, -1, 4, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0]. \n",
        "\n",
        "By convention, the irreducible factors are listed in reverse lexicographical order. To recover the individual factors from a (9n,)-shaped array storing a decomposition we need to split it into n equal parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ipdm_BV-gc"
      },
      "source": [
        "for i in range(1, 5):\n",
        "  with open('polys_int_set_{}.pkl'.format(i), 'rb') as file:\n",
        "    globals()['polys_%s' % i], _ = pk.load(file)\n",
        "\n",
        "  with open('decomp_int_set_{}.pkl'.format(i), 'rb') as file:\n",
        "    globals()['decomps_%s' % i], _ = pk.load(file)\n",
        "\n",
        "polys = polys_1 + polys_2 + polys_3 + polys_4\n",
        "decomps = decomps_1 + decomps_2 + decomps_3 + decomps_4\n",
        "\n",
        "for i in range(1, 5):\n",
        "  del  globals()['polys_%s' % i]\n",
        "  del  globals()['decomps_%s' % i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KLQ7KgOW0uZ"
      },
      "source": [
        "Let's get a visual on some attributes of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhWEnqFQXGNO"
      },
      "source": [
        "num_factors = []\n",
        "degrees = []\n",
        "\n",
        "for poly in polys:\n",
        "  degree = len(np.trim_zeros(poly, 'b')) - 1\n",
        "  degrees.append(degree)\n",
        "\n",
        "for decomp in decomps:\n",
        "  splits = len(decomp) / 9\n",
        "  num_factors.append(splits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "fwuLYCC5cBJe",
        "outputId": "72e3a0f0-3b00-4c66-80e5-9128abfd8f76"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', palette=sns.diverging_palette(145, 400, s=20, l=30,\n",
        "                                                      n=8))\n",
        "\n",
        "fig, ax = plt.subplots(2, 1)\n",
        "ax[0].hist(degrees, density=True, bins=range(9), alpha=0.7, color='crimson',\n",
        "           align='right')\n",
        "ax[1].hist(num_factors, density=True, bins=range(1,9), alpha=0.7, \n",
        "           color='steelblue', align='left')\n",
        "ax[0].set(xlabel='Degree', ylabel='Percentage of dataset')\n",
        "ax[1].set(xlabel='Number of factors', ylabel='Percentage of dataset')\n",
        "ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
        "ax[1].yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
        "fig.set_size_inches(10, 10)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ZxUY8tmkCr"
      },
      "source": [
        "More than 90% of the polynomials in the dataset have degree 8. This is what we'd expect since there are about 11 times more polynomials of degree 8 than polynomials of degree 7 for the restricted range of coefficients [-5, 5]. On the other hand over 80% of the polynomials in the dataset are irreducible, i.e. their decompositions consist of the polynomials themselves. So we will only consider the experiment a success if we can achieve an accuracy much better than 80%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TRYArRUWue"
      },
      "source": [
        "# facilities to pass from strings data to ints and vice-versa\n",
        "def to_string(poly):\n",
        "  poly = poly.astype('int')\n",
        "  poly = [str(number) for number in poly]\n",
        "  string_poly = [\"\"] * (2*len(poly) - 1)\n",
        "  string_poly[0::2] = poly\n",
        "  string_poly[1::2] = '+'*(len(poly) - 1) \n",
        "  return ''.join(string_poly)\n",
        "\n",
        "def to_int_array(poly):\n",
        "  poly = poly.split('+')\n",
        "  poly = np.asarray([int(number) for number in poly])\n",
        "  return np.trim_zeros(poly, 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLOrVWa1UWug"
      },
      "source": [
        "for index, poly in enumerate(polys):\n",
        "  poly = to_string(poly)\n",
        "  polys[index] = poly\n",
        "\n",
        "# find individual factors in decompositions\n",
        "for index, de in enumerate(decomps):\n",
        "  splits = int(len(de) / 9)\n",
        "  factors = np.split(de, splits)\n",
        "  factors = [np.trim_zeros(factor, 'b') for factor in factors]\n",
        "  factors = [to_string(factor) for factor in factors]\n",
        "  string_factors = [\"\"]*(2*splits - 1)\n",
        "  string_factors[0::2] = factors\n",
        "  string_factors[1::2] = '|'*(splits - 1)\n",
        "  string_factors = ''.join(string_factors)\n",
        "  decomps[index] = string_factors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAXHXxShUlDL"
      },
      "source": [
        "Now we prepare the training data for training. We will encode the data character by character, treating + and - as label classes. We need to be careful about memory management due to the large size of the data and the 24GB cap of usable RAM in Google Colab Pro.\n",
        "\n",
        "Recall that during training seq2seq models require two inputs: the input to the encoder layer, in this case the polynomials we wish to decompose, and the input to the decoder layer which is the target output shifted one timestep into the past. The idea is to train the model to make a prediction for timestep t + 1 having seen timesteps 1...t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUFahoAHU4-J"
      },
      "source": [
        "# Define dictionary for encoding\n",
        "token_dict = {}\n",
        "token_dict['pad'] = 0\n",
        "for i in (list(range(0, 10)) + ['+', '-', '|', 'start', 'end']):\n",
        "  token_dict[str(i)] = len(token_dict)\n",
        "rev_token_dict = {v:k for k, v in token_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuQXo0yrU4-K"
      },
      "source": [
        "# determine maximum input and output lengths\n",
        "max_len_decomps = 0\n",
        "for de in decomps:\n",
        "  if len(de) > max_len_decomps:\n",
        "    max_len_decomps = len(de)\n",
        "\n",
        "max_len_polys = 0\n",
        "for poly in polys:\n",
        "  if len(poly) > max_len_polys:\n",
        "    max_len_polys = len(poly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQf-IcW9Uhki"
      },
      "source": [
        "data_len = len(polys)\n",
        "\n",
        "encoder_inputs = np.zeros((data_len, max_len_polys + 2), dtype='int8')\n",
        "decoder_inputs = np.zeros((data_len, max_len_decomps + 2), dtype='int8')\n",
        "decoder_outputs = np.zeros((data_len, max_len_decomps + 2), dtype='int8')\n",
        "\n",
        "for index in range(data_len):\n",
        "  posn = data_len - index - 1\n",
        "\n",
        "  # construct encoder input\n",
        "  poly = polys.pop()\n",
        "  poly = np.asarray([token_dict[char] for char in poly] + \n",
        "                    [token_dict['end']], 'int')\n",
        "  poly = np.pad(poly, (1, max_len_polys + 1 - len(poly)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  encoder_inputs[posn, :] = poly\n",
        "\n",
        "  # construct decoder input and output\n",
        "  de = decomps.pop()\n",
        "  de = np.asarray([token_dict[char] for char in de] + \n",
        "                  [token_dict['end']], 'int')\n",
        "  de_1 = np.pad(de, (1, max_len_decomps + 1 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  de_2 = np.pad(de, (0, max_len_decomps + 2 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  decoder_inputs[posn, :] = de_1\n",
        "  decoder_outputs[posn, :] = de_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzbwErjxbJbO"
      },
      "source": [
        "Now we construct the model. Hyperparameters were chosen based on standard values suggested in the literature. This is because each training iteration takes several hours and thus hyperparameter-tuning would be prohibitively time-consuming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzhcM_i5twEB",
        "outputId": "c9ab59be-7a9e-4f22-f524-d2cd66914b23"
      },
      "source": [
        "model = get_model(\n",
        "    token_num=len(token_dict),\n",
        "    embed_dim=512,\n",
        "    encoder_num=6,\n",
        "    decoder_num=6,\n",
        "    head_num=8,\n",
        "    hidden_dim=2048,\n",
        "    attention_activation='relu',\n",
        "    feed_forward_activation='relu')\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Token-Embedding (EmbeddingRet)  [(None, None, 512),  8192        Encoder-Input[0][0]              \n",
            "                                                                 Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Embedding (TrigPosEmbed (None, None, 512)    0           Token-Embedding[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-Embedding[0][0]          \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, None, 512)    0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, None, 512)    0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, None, 512)    0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, None, 512)    0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, None, 512)    0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 512)    1050624     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 512)    0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Embedding (TrigPosEmbed (None, None, 512)    0           Token-Embedding[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 512)    1024        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, None, 512)    2099712     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-Embedding[0][0]          \n",
            "                                                                 Decoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, None, 512)    0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, None, 512)    1024        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Add (Add) (None, None, 512)    0           Decoder-1-MultiHeadQueryAttention\n",
            "                                                                 Decoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Add (Add) (None, None, 512)    0           Decoder-2-MultiHeadQueryAttention\n",
            "                                                                 Decoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-3-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-3-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-3-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-FeedForward-Add (Add) (None, None, 512)    0           Decoder-3-MultiHeadQueryAttention\n",
            "                                                                 Decoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-3-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-4-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-4-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-4-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-FeedForward-Add (Add) (None, None, 512)    0           Decoder-4-MultiHeadQueryAttention\n",
            "                                                                 Decoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-4-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-5-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-5-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-5-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-FeedForward-Add (Add) (None, None, 512)    0           Decoder-5-MultiHeadQueryAttention\n",
            "                                                                 Decoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-5-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-6-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-6-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-FeedForward (FeedForw (None, None, 512)    2099712     Decoder-6-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-FeedForward-Add (Add) (None, None, 512)    0           Decoder-6-MultiHeadQueryAttention\n",
            "                                                                 Decoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-6-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Output (EmbeddingSim)   (None, None, 16)     16          Decoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Token-Embedding[1][1]            \n",
            "==================================================================================================\n",
            "Total params: 44,146,704\n",
            "Trainable params: 44,146,704\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qkZCcUJcUmE"
      },
      "source": [
        "The model is trained for just 1 epoch, which takes 10h on Google Colab Pro. There is no validation split obviously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RQBbCoMJzan",
        "outputId": "80b6f3ec-73c0-4749-839e-f8a4bdf64b7b"
      },
      "source": [
        "model.fit(\n",
        "    x=[encoder_inputs, decoder_inputs],\n",
        "    y=decoder_outputs,\n",
        "    epochs=1,\n",
        "    batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77344/77344 [==============================] - 36631s 474ms/step - loss: 0.0300 - accuracy: 0.9843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f714fb74590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvbu4dB2BPJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94001fb2-7a05-4b8b-af47-c085bd32367c"
      },
      "source": [
        "model.save('./trained_model')\n",
        "# to reload model:\n",
        "# model = keras.models.load_model('./trained_model', \n",
        "#                                   custom_objects=get_custom_objects())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Encoder-Input, Decoder-Input with unsupported characters which will be renamed to encoder_input, decoder_input in the SavedModel.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./trained_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./trained_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqRVZf-u8ZCZ"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Let's see how will our trained neural network does on never-seen data. We load the test data (200K samples) and preprocess it like before. The test data was generated in the same way as the training data, i.e. by random sampling in Mathematica. Since our training data represents such a small fraction of the input space (~1.7%), the expected overlap between the training and test sets is negligible and we can assume that no data snooping will occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEywiBxOxTHI"
      },
      "source": [
        "with open('test_polys.pkl'.format(i), 'rb') as file:\n",
        "  test_polys = pk.load(file)\n",
        "with open('test_decomps.pkl'.format(i), 'rb') as file:\n",
        "  test_decomps = pk.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYS6G4d-arL7"
      },
      "source": [
        "for index, poly in enumerate(test_polys):\n",
        "  poly = to_string(poly)\n",
        "  test_polys[index] = poly\n",
        "\n",
        "for index, de in enumerate(test_decomps):\n",
        "  splits = int(len(de) / 9)\n",
        "  factors = np.split(de, splits)\n",
        "  factors = [np.trim_zeros(factor, 'b') for factor in factors]\n",
        "  factors = [to_string(factor) for factor in factors]\n",
        "  string_factors = [\"\"]*(2*splits - 1)\n",
        "  string_factors[0::2] = factors\n",
        "  string_factors[1::2] = '|'*(splits - 1)\n",
        "  string_factors = ''.join(string_factors)\n",
        "  test_decomps[index] = string_factors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auIGF8j38s3K"
      },
      "source": [
        "Let's just make sure that the maximal lengths of our test data does not exceed those seen in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9lXCksoUjXi"
      },
      "source": [
        "max_len_polys_test = 0\n",
        "for poly in test_polys:\n",
        "  if len(poly) > max_len_polys_test:\n",
        "    max_len_polys_test = len(poly)\n",
        "\n",
        "max_len_decomps_test = 0\n",
        "for de in test_decomps:\n",
        "  if len(de) > max_len_decomps_test:\n",
        "    max_len_decomps_test = len(de)\n",
        "\n",
        "assert max_len_polys >= max_len_polys_test\n",
        "assert max_len_decomps >= max_len_decomps_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjr7OPHw8_sa"
      },
      "source": [
        "Let's prepare the test data like we did for the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDSD5G4YYBQB"
      },
      "source": [
        "data_len = len(test_polys)\n",
        "\n",
        "test_encoder_inputs = np.zeros((data_len, max_len_polys + 2), dtype='int8')\n",
        "test_decoder_inputs = np.zeros((data_len, max_len_decomps + 2), dtype='int8')\n",
        "test_decoder_outputs = np.zeros((data_len, max_len_decomps + 2), dtype='int8')\n",
        "\n",
        "for index in range(data_len):\n",
        "  posn = data_len - index - 1\n",
        "\n",
        "  # construct encoder input\n",
        "  poly = test_polys.pop()\n",
        "  poly = np.asarray([token_dict[char] for char in poly] + \n",
        "                    [token_dict['end']], 'int')\n",
        "  poly = np.pad(poly, (1, max_len_polys + 1 - len(poly)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  test_encoder_inputs[posn, :] = poly\n",
        "\n",
        "  # construct decoder input and output\n",
        "\n",
        "  # first preprocess decomposition\n",
        "  de = test_decomps.pop()\n",
        "  de = np.asarray([token_dict[char] for char in de] + \n",
        "                  [token_dict['end']], 'int')\n",
        "  de_1 = np.pad(de, (1, max_len_decomps + 1 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  de_2 = np.pad(de, (0, max_len_decomps + 2 - len(de)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  test_decoder_inputs[posn, :] = de_1\n",
        "  test_decoder_outputs[posn, :] = de_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiP-A7Ij9nUj"
      },
      "source": [
        "First we test our neural network in the same way we trained it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-II4SZubCwv",
        "outputId": "5e85a2c3-b143-450a-b7dd-a8085a2d7027"
      },
      "source": [
        "model.evaluate([test_encoder_inputs, test_decoder_inputs], test_decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6250/6250 [==============================] - 187s 27ms/step - loss: 0.0057 - accuracy: 0.9979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0057304780930280685, 0.9979027509689331]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVtxteKADCC"
      },
      "source": [
        "Not bad! Next we will test our model at inference, i.e. without any knowledge of the correct decoder inputs. We will do this with a beam search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syeC_CYa6oHr"
      },
      "source": [
        "def beam_search(model, src_input, k=1, max_len=max_len_decomps + 2):\n",
        "    # k_beam will be updated to contain best k beams at each timestep\n",
        "    src_input = src_input.reshape((1,max_len_polys + 2))\n",
        "    k_beam = [(0, np.zeros(max_len))]\n",
        "\n",
        "    # l : point on target sentence to predict\n",
        "    for l in range(max_len):\n",
        "        all_k_beams = []\n",
        "        for prob, sent_predict in k_beam:\n",
        "          if token_dict['end'] in sent_predict:\n",
        "            all_k_beams.append((prob, sent_predict))\n",
        "          else:\n",
        "            decoder_input = np.concatenate([[token_dict['start']], \n",
        "                                          sent_predict[:-1]]).reshape(\n",
        "                                              (1, max_len))\n",
        "            predicted = model.predict([src_input, decoder_input])[0]\n",
        "            possible_k = predicted[l].argsort()[-k:][::-1]\n",
        "      \n",
        "            for next_wid in possible_k:\n",
        "              all_k_beams.append(\n",
        "                  ((prob*l - np.log(predicted[l][next_wid]))/(l+1), \n",
        "                   np.concatenate([sent_predict[:l], np.asarray([next_wid]),\n",
        "                                   np.zeros(max_len - l - 1)])))\n",
        "        k_beam = sorted(all_k_beams, key=lambda tup:tup[0])[:k]\n",
        "\n",
        "    return k_beam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfNkvJwF-iWy"
      },
      "source": [
        "We test the accuracy of the model using different beam sizes at inference on 1000 test inputs. We consider the neural network as having solved a problem if the solution is in the final beam.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pI0p5fHCyom",
        "outputId": "68d79866-398d-4217-bb04-dd34e569a9cb"
      },
      "source": [
        "num_samples = 1000\n",
        "for k in [1, 2, 3, 5]:\n",
        "  truth_list = []\n",
        "  for index, poly in enumerate(test_encoder_inputs[:num_samples]):\n",
        "    for _, soln in beam_search(model=model, src_input=poly, k=k):\n",
        "      if np.array_equal(test_decoder_outputs[index], soln):\n",
        "        truth_list.append(True)\n",
        "        break\n",
        "    if len(truth_list) < index + 1:\n",
        "      truth_list.append(False)\n",
        "  print('Accuracy with beam size {}:'.format(k),\n",
        "        '{}/{}'.format(truth_list.count(True), num_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with beam size 1: 971/1000\n",
            "Accuracy with beam size 2: 985/1000\n",
            "Accuracy with beam size 3: 988/1000\n",
            "Accuracy with beam size 5: 990/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq8ymmrOpT5I"
      },
      "source": [
        "Accuracy steadily improved with beam size. This is in contrast to NLP applications where increased beam size can sometimes be detrimental to accuracy. Lample and Charton also observed substantial improvements as beam size increased. This suggests that for the special use case of symbolic mathematics, higher beam size is generally better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6Au7C6FwYa0"
      },
      "source": [
        "Finally, I wanted to see how well would the model generalize to a range of input characters it has never seen. For that purpose we generate a set of 100 polynomials with coefficients between -9 and 9, making sure that not all coefficients are between -5 and 5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrSkf6ziwyo4"
      },
      "source": [
        "from math import gcd\n",
        "import random\n",
        "from random import choices\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "# calculates gcd of a list of numbers\n",
        "def gcd_v(array):\n",
        "  array = array.astype('int')\n",
        "  if len(array) == 2:\n",
        "    return gcd(array[0], array[1])\n",
        "  return gcd(gcd_v(array[:-1]), array[-1])\n",
        "\n",
        "# define new test data\n",
        "new_polys = []\n",
        "while len(new_polys) < 100:\n",
        "  poly = choices(range(-9, 10), k=9)\n",
        "  if max(poly) <= 5 and min(poly) >= -5:\n",
        "    continue\n",
        "  else:\n",
        "    poly = np.asarray(poly, 'int')\n",
        "    poly = poly / gcd_v(poly)\n",
        "    poly = to_string(poly)\n",
        "    if len(poly) > max_len_polys:\n",
        "      continue\n",
        "    else:\n",
        "      new_polys.append(poly)\n",
        "\n",
        "new_polys_inputs = np.zeros((len(new_polys), max_len_polys + 2))\n",
        "\n",
        "for index, poly in enumerate(new_polys):\n",
        "  posn = len(new_polys) - index - 1\n",
        "  poly = np.asarray([token_dict[char] for char in poly] + \n",
        "                    [token_dict['end']], 'int')\n",
        "  poly = np.pad(poly, (1, max_len_polys + 1 - len(poly)), 'constant', \n",
        "                constant_values=(token_dict['start'], token_dict['pad']))\n",
        "  new_polys_inputs[posn, :] = poly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOMy7m6FMkpV"
      },
      "source": [
        "We will simply check whether the neural network's solutions are 1. valid and 2. give factors that multiply to the input polynomials, nevermind the irreducibility of the factors. Let's define a function verify_soln() for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ9yV0vRN9Wt"
      },
      "source": [
        "# implements polynomial multiplication with the convention we set down\n",
        "def polymul(p1, p2):\n",
        "  prod_size = len(p1) + len(p2) - 1\n",
        "  p1 = np.pad(p1, (0, prod_size - len(p1)))\n",
        "  p2 = np.pad(p2, (0, prod_size - len(p2)))\n",
        "  prod = np.zeros(prod_size)\n",
        "  for k in range(prod_size):\n",
        "    for j in range(k + 1):\n",
        "      prod[k] += p1[j] * p2[k - j]\n",
        "  prod = np.trim_zeros(prod, 'b')\n",
        "  return prod\n",
        "\n",
        "def verify_soln(poly, soln):\n",
        "  # preprocess polynomial\n",
        "  poly = np.trim_zeros(poly, 'b')[1:-1]\n",
        "  poly = poly.tolist()\n",
        "  poly = ''.join([rev_token_dict[member] for member in poly])\n",
        "  poly = to_int_array(poly)\n",
        "\n",
        "  # preprocess solution\n",
        "  soln = np.trim_zeros(soln, 'b')\n",
        "  if soln[-1] != token_dict['end']:\n",
        "    return False\n",
        "  soln = soln[:-1].tolist()\n",
        "  soln = ''.join([rev_token_dict[member] for member in soln])\n",
        "  factors = soln.split('|')\n",
        "\n",
        "  # proposed solution may not be semantically valid\n",
        "  try:\n",
        "    factors = [to_int_array(factor) for factor in factors]\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "  # calculate product given by solution\n",
        "  product = np.ones(1)\n",
        "  while len(factors) > 0:\n",
        "    factor = factors.pop()\n",
        "    product = polymul(product, factor)\n",
        "\n",
        "  if np.array_equal(poly, product):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tjEqbX42lh4"
      },
      "source": [
        "Let's try with beam sizes 1, 3 and 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8xQDm4f2XnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bccd68f-ab54-4e2b-8357-7612d18d30f2"
      },
      "source": [
        "for k in [1, 3, 5]:\n",
        "  truth_list = []\n",
        "  for index, poly in enumerate(new_polys_inputs):\n",
        "    for _, soln in beam_search(model=model, src_input=poly, k=k):\n",
        "      if verify_soln(poly, soln):\n",
        "        truth_list.append(True)\n",
        "        break\n",
        "    if len(truth_list) < index + 1:\n",
        "      truth_list.append(False)\n",
        "  print('Accuracy with beam size {}:'.format(k),\n",
        "        '{}/{}'.format(truth_list.count(True), len(new_polys_inputs)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with beam size 1: 0/100\n",
            "Accuracy with beam size 3: 0/100\n",
            "Accuracy with beam size 5: 0/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GLilFDW8hKB"
      },
      "source": [
        "The neural network doesn't generalize at all! This is not too surprising; the embedding layer does not actually \"understand\" numerical quantities and so there is no reason it should generalize to a range of input coefficients it has never seen. This said, there is a caveat: the model would have a better chance at generalization if had seen every digit and positional value at least once, as opposed to only seeing the digits 0-5 and their negatives. For instance, it might be able to correctly interpret the number 15 if it sees 10 and 5 during training. However, naively extending the range of input coefficients to [-10, 10] would require a larger training dataset since the input space would be far larger (about 700B possible inputs). I expect that by adding a neural arithmetic logic unit (see [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)) to the model we would see significant improvements to never-seen ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrnFLktCHZ-j"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Athough the neural network performed well on test data drawn from the input coefficient range on which it was trained, it completely failed to generalize to test data drawn from a never-seen coefficient range. It appears to me the issue is that a naive transformer model is not sufficient: we need layers that provide the neural network with a \"sense\" of numeracy. NALUs seem promising in that regard. Furthermore it seems that without such improvements to the transformer model, datasets would need to be extremely large to obtain adequate accuracy over an acceptably large input space.\n",
        "\n",
        "Nonetheless, the fact that the neural network displayed such great accuracy on the restricted input space we considered, this after having seen fewer than 2% of all possible inputs, is very promising. It shows that neural networks are indeed capable of learning non-trivial mathematical relationships exceptionally well. I therefore expect them to continue finding applications in symbolic mathematics, even though they are not the panacea they may appear to be at first glance. I am very interested in exploring ways of improving the transformer model for this special use case in the future."
      ]
    }
  ]
}
